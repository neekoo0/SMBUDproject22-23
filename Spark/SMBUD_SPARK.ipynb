{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d120afb9-fe20-4bf8-9998-51bfcefe12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "771d0a08-c7fc-40ba-a469-6bce668947df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"SMBUD_project\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f7d327-0196-4366-b1d3-9114db1cdda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define article custom schema\n",
    "schemaArticle = StructType([\n",
    "\tStructField('_id', StringType(), True),\n",
    "\tStructField('title', StringType(), True),\n",
    "\tStructField('authors',\n",
    "\t\tArrayType(\n",
    "\t\tStructType([\n",
    "\t\t\t StructField('idAuth', StringType(), True),\n",
    "\t\t\t StructField('org', StringType(), True)\n",
    "\t\t]), True)\n",
    "\t),\n",
    "\tStructField('n_citation', IntegerType(), True), \n",
    "\tStructField('abstract', StringType(), True), \n",
    "\tStructField('doi', StringType(), True),\n",
    "\tStructField('keywords', ArrayType(StringType()), True),\n",
    "\tStructField('isbn', StringType(), True),\n",
    "\tStructField('page_start', StringType(), True),\n",
    "\tStructField('page_end', StringType(), True),\n",
    "\tStructField('year', IntegerType(), True),\n",
    "\tStructField('fos', ArrayType(StringType()), True),\n",
    "\tStructField('references', ArrayType(StringType()), True),\n",
    "\tStructField('venue',\n",
    "\t\tStructType([\n",
    "\t\t\t StructField('raw', StringType(), True),\n",
    "\t\t\t StructField('type', IntegerType(), True),\n",
    "\t\t\t StructField('issue', StringType(), True),\n",
    "\t\t\t StructField('volume', StringType(), True),\n",
    "\t\t\t StructField('publisher', StringType(), True)\n",
    "\t\t])\n",
    "\t),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a9536e-d248-4dd9-9afd-64515f9a79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we decided to use import from schema to explicitly show data structure\n",
    "df_articles = spark.read.schema(schemaArticle).json(\"./dblp_sample_filtered_spark.json\", multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deaef0ca-2e9f-4ba6-b3cc-9456c199502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue, volume and publisher attributes inside venue are moved back in the root structure and removed from the inner struct\n",
    "df_articles = df_articles.withColumn(\"issue\", f.col(\"venue.issue\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"volume\", f.col(\"venue.volume\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"publisher\", f.col(\"venue.publisher\")) \\\n",
    "\t\t\t\t\t\t.withColumn(\"venue\", f.col(\"venue\").dropFields(\"issue\", \"volume\", \"publisher\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd3e637-2843-4a34-b71d-9d5cea4e8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VENUES COLLECTION\n",
    "#A new dataframe is created with attributes of venue and the _id of the article\n",
    "#then it is all grouped by venue attributes and a list of the articles id for each venue is created\n",
    "#finally we drop rows with null raw to delete inconsistent tuple\n",
    "df_venues = df_articles.select(\"venue.raw\", \"venue.type\", \"_id\") \\\n",
    "\t\t\t\t\t\t.groupBy(\"raw\", \"type\") \\\n",
    "\t\t\t\t\t\t.agg(f.collect_list(\"_id\").alias(\"artIds\")) \\\n",
    "\t\t\t\t\t\t.dropna(subset=[\"raw\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ef4ef3-ee77-4742-b378-8a90b8ae4e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can keep only the raw attribute of the venue\n",
    "df_articles = df_articles.withColumn(\"venue_raw\", f.col(\"venue.raw\")).drop(\"venue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8868f13b-8b1d-4b0e-8c0d-a7cb2721ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we now add a generated field inside venues collection\n",
    "#for each venue a random city is selected that should represent the place where the venue was held\n",
    "citiesList = [\"New York\", \"London\", \"Paris\", \"Berlin\", \"Madrid\", \"Rome\", \"Dublin\", \"Copenhagen\", \"Vienna\", \"Amsterdam\", \"Brussels\", \"Lisbon\", \"Prague\", \"Athens\", \"Budapest\", \"Warsaw\", \"Zurich\", \"Luxembourg\", \"Oslo\", \"Stockholm\", \"Helsinki\", \"Moscow\", \"Istanbul\", \"Kiev\", \"Minsk\", \"Belgrade\", \"Bucharest\", \"Sofia\", \"Tallinn\", \"Riga\", \"Vilnius\", \"Tbilisi\", \"Yerevan\", \"Baku\", \"Dubai\", \"Abu Dhabi\", \"Doha\", \"Manama\", \"Muscat\", \"Riyadh\", \"Jeddah\", \"Mecca\", \"Medina\", \"Kuala Lumpur\", \"Singapore\", \"Hong Kong\", \"Shanghai\", \"Beijing\", \"Tokyo\", \"Seoul\", \"Bangkok\", \"Manila\"]\n",
    "cities = f.array([f.lit(city) for city in citiesList])\n",
    "df_venues = df_venues.withColumn(\"city\", cities[(f.rand(seed=42) * len(citiesList)).cast(\"int\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "586d555c-4412-471d-90dc-25c6fd9ada30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema for the DataFrame of Authors\n",
    "schemaAuthors = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"nationality\", StringType(), True),\n",
    "    StructField(\"articles\", ArrayType(StringType(), True), True),\n",
    "    StructField(\"bio\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"orcid\", StringType(), True),\n",
    "    StructField(\"dob\", TimestampType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a40b4b24-c483-4912-8278-7e98928f4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTHORS COLLECTION\n",
    "#We simply import from json with specified schema and the conversion from string to timestamp is applied\n",
    "df_authors = spark.read.schema(schemaAuthors).json(\"./dblp_sample_reverted_filtered_spark.json\", multiLine=True)\n",
    "df_authors = df_authors.withColumn(\"dateofbirth\", f.to_timestamp(df_authors[\"dob\"], \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
    "\t\t\t\t\t\t.drop(\"dob\") \\\n",
    "\t\t\t\t\t\t.withColumnRenamed(\"dateofbirth\", \"dob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396efc6f-7824-4e47-89cd-24b973d1cc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- idAuth: string (nullable = true)\n",
      " |    |    |-- org: string (nullable = true)\n",
      " |-- n_citation: integer (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- doi: string (nullable = true)\n",
      " |-- keywords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- isbn: string (nullable = true)\n",
      " |-- page_start: string (nullable = true)\n",
      " |-- page_end: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- fos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- issue: string (nullable = true)\n",
      " |-- volume: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- venue_raw: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- articles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- bio: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- orcid: string (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- raw: string (nullable = true)\n",
      " |-- type: integer (nullable = true)\n",
      " |-- artIds: array (nullable = false)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+--------------------+----+--------------------+------------+\n",
      "|                 raw|type|              artIds|        city|\n",
      "+--------------------+----+--------------------+------------+\n",
      "|2006 IEEE Interna...|   0|[53e9a281b7602d97...|     Yerevan|\n",
      "|2007 IEEE Interna...|   0|[53e9a317b7602d97...|   Bucharest|\n",
      "|2009 IEEE INTERNA...|   0|[53e99fd6b7602d97...|Kuala Lumpur|\n",
      "|2010 Internationa...|   0|[53e99fddb7602d97...|      Athens|\n",
      "|25 Years of Model...|   0|[53e99fe9b7602d97...|       Dubai|\n",
      "|2ND INTERNATIONAL...|null|[53e99f94b7602d97...|   Bucharest|\n",
      "|                 4OR|   0|[53e99fe4b7602d97...|      Manila|\n",
      "|7TH INTERNATIONAL...|null|[53e9a31fb7602d97...|      Berlin|\n",
      "|A Quarterly Journ...|   1|[53e99fe4b7602d97...|     Bangkok|\n",
      "|              A2CWiC|   0|[53e99fddb7602d97...|       Mecca|\n",
      "+--------------------+----+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_articles.printSchema()\n",
    "df_authors.printSchema()\n",
    "df_venues.printSchema()\n",
    "df_venues.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b438276-9744-4321-96d9-3daf1b9ee18d",
   "metadata": {},
   "source": [
    "# Data creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddddf77",
   "metadata": {},
   "source": [
    "##### 1 - Insert new author Emanuele Della Valle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cabc87d0-de4e-498a-bf66-4b827c2264a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a new Row object with the values for the new author\n",
    "new_author = Row(\n",
    "    _id=\"638db170ae9ea0d19fad7a79\",              \n",
    "    name=\"Emanuele Delle Valle \",\n",
    "    nationality=\"it\",\n",
    "    # Set values for any other required columns\n",
    "    articles=[],\n",
    "    bio=\"Emanuele Della Valle holds a PhD in Computer Science from the \\\n",
    "        Vrije Universiteit Amsterdam and a Master degree in Computer Science\\\n",
    "        and Engineering from Politecnico di Milano. He is associate professor\\\n",
    "        at the Department of Electronics, Information and Bioengineering of\\\n",
    "        the Politecnico di Milano.\",\n",
    "    email=\"emanuele.dellavalle@gmail.com \",\n",
    "    orcid=\"0000-0002-5176 -5885\",\n",
    "    dob= datetime.strptime(\"March 7, 1975\", \"%B %d, %Y\")  # Create a datetime object for the author's date of birth\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_authors = df_authors.union(spark.createDataFrame([new_author], schema = schemaAuthors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb8c6a04-ef96-4454-b41c-f3b84d34e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                 _id|                name|nationality|articles|                 bio|               email|               orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "|638db170ae9ea0d19...|Emanuele Delle Va...|         it|      []|Emanuele Della Va...|emanuele.dellaval...|0000-0002-5176 -5885|1975-03-07 00:00:00|\n",
      "+--------------------+--------------------+-----------+--------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b63dc06",
   "metadata": {},
   "source": [
    "##### 2 - Insert new publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378c58cb-c44f-4357-938b-783580ab8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_authors =  [Row(\"638db170ae9ea0d19fad7a79\", \"Politecnico di Milano\"), Row(\"638db170ae9ea0d19fad7a7a\", \"Politecnico di Milano\")] \n",
    "\n",
    "new_article = Row(\n",
    "    _id=\"638db237d794b76f45c77916\",\n",
    "    title=\"An extensive study of C-SMOTE, a Continuous Synthetic Minority Oversampling Technique for Evolving Data Streams\",\n",
    "    authors=new_authors,\n",
    "    n_citation=3,\n",
    "    abstract = \"Streaming Machine Learning (SML) studies algorithms that update their models,\\\n",
    "        given an unbounded and often non-stationary flow of data performing a single pass. Online \\\n",
    "        class imbalance learning is a branch of SML that combines the challenges of both class imbalance\\\n",
    "        and concept drift. In this paper, we investigate the binary classification problem by rebalancing\\\n",
    "        an imbalanced stream of data in the presence of concept drift, accessing one sample at a time.\",\n",
    "    doi=\"10.1016/j.eswa.2022.116630\",\n",
    "    keywords=[\"Evolving Data Stream\",\"Streaming\",\"Concept drift\",\"Balancing\"],\n",
    "    isbn=\"123-4-567-89012-3\",\n",
    "    page_start=\"39\",\n",
    "    page_end=\"46\",\n",
    "    year=2022,\n",
    "    fos=[\"Computer Science\",\"Stream Reasoning\",\"Big Data\"],\n",
    "    references=[\"53e99fe4b7602d97028bf743\",\"53e99fddb7602d97028bc085\"],\n",
    "    issue=\"1\",\n",
    "    volume=\"196\",\n",
    "    publisher=\"Elsevier\",\n",
    "    venue_raw=\"ESA\"\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_articles = df_articles.union(spark.createDataFrame([new_article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59135eb0-f892-45e5-9d59-80a1910b8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "|                 _id|               title|             authors|n_citation|            abstract|                 doi|            keywords|             isbn|page_start|page_end|year|                 fos|          references|issue|volume|publisher|venue_raw|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "|638db237d794b76f4...|An extensive stud...|[{638db170ae9ea0d...|         3|Streaming Machine...|10.1016/j.eswa.20...|[Evolving Data St...|123-4-567-89012-3|        39|      46|2022|[Computer Science...|[53e99fe4b7602d97...|    1|   196| Elsevier|      ESA|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+-----------------+----------+--------+----+--------------------+--------------------+-----+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_articles.filter(f.col(\"_id\") == \"638db237d794b76f45c77916\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099da3b",
   "metadata": {},
   "source": [
    "##### 3 - Insert new venue \"ESA\", assuming it is not present in the db yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b314f663-c6b7-44bb-a5db-e6203f2e42b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_venue = Row(                  \n",
    "    raw=\"ESA\", \n",
    "    type=1,\n",
    "    artIds=[\"638db237d794b76f45c77916\"],\n",
    "    city=\"Montreal\"\n",
    ")\n",
    "\n",
    "# Add the new row to the DataFrame\n",
    "df_venues = df_venues.union(spark.createDataFrame([new_venue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "499ef802-20c7-417a-b192-1722f1db1309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+--------+\n",
      "|raw|type|              artIds|    city|\n",
      "+---+----+--------------------+--------+\n",
      "|ESA|   1|[638db237d794b76f...|Montreal|\n",
      "+---+----+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_venues.filter(f.col(\"raw\") == \"ESA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9f7a2",
   "metadata": {},
   "source": [
    "##### 4 - Adding the new article to both authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b70a1e1-de68-4cce-93c9-dfaed0a22de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the new article to the new author\n",
    "\n",
    "df_authors = df_authors.withColumn(\n",
    "    \"articles\",\n",
    "    f.when(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\",\n",
    "        f.array_union(df_authors.articles, f.array(f.lit(\"638db237d794b76f45c77916\"))))\\\n",
    "    .when(f.col(\"_id\") == \"638db170ae9ea0d19fad7a7a\",\n",
    "        f.array_union(df_authors.articles, f.array(f.lit(\"638db237d794b76f45c77916\"))))\n",
    "    .otherwise(f.col(\"articles\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4b9bcb-b893-4a4e-b38f-5e4155ea5a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                 _id|                name|nationality|            articles|                 bio|               email|               orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|638db170ae9ea0d19...|Emanuele Delle Va...|         it|[638db237d794b76f...|Emanuele Della Va...|emanuele.dellaval...|0000-0002-5176 -5885|1975-03-07 00:00:00|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show() #checking only Emanuele della Valle since the other author hasn't been inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fedb261",
   "metadata": {},
   "source": [
    "##### 5 - Incrementing n_citations by 1 of cited articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c17bbcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fe4b7602d970...|        12|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fddb7602d970...|         2|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking previous n_citation\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\").select(\"_id\",\"n_citation\").show()\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fddb7602d97028bc085\").select(\"_id\",\"n_citation\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f760314c-506e-47ed-b69a-71bcb7f0fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increment number of citations\n",
    "df_articles = df_articles.withColumn(\n",
    "    \"n_citation\",\n",
    "    f.when(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\",\n",
    "       df_articles.n_citation+1) \\\n",
    "    .when(f.col(\"_id\") == \"53e99fddb7602d97028bc085\",\n",
    "       df_articles.n_citation+1)   \n",
    "    .otherwise(f.col(\"n_citation\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb4991b-0a8c-46fa-af47-0e9d5075b698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fe4b7602d970...|        13|\n",
      "+--------------------+----------+\n",
      "\n",
      "+--------------------+----------+\n",
      "|                 _id|n_citation|\n",
      "+--------------------+----------+\n",
      "|53e99fddb7602d970...|         3|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking updated n_citation\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fe4b7602d97028bf743\").select(\"_id\", \"n_citation\").show()\n",
    "df_articles.filter(f.col(\"_id\") == \"53e99fddb7602d97028bc085\").select(\"_id\", \"n_citation\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54411d",
   "metadata": {},
   "source": [
    "##### 6 - Deleting an author from the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac948e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|                 _id|                name|nationality|            articles|                 bio|               email|               orcid|                dob|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|638db170ae9ea0d19...|Emanuele Delle Va...|         it|[638db237d794b76f...|Emanuele Della Va...|emanuele.dellaval...|0000-0002-5176 -5885|1975-03-07 00:00:00|\n",
      "+--------------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "\n",
      "+---+----+-----------+--------+---+-----+-----+---+\n",
      "|_id|name|nationality|articles|bio|email|orcid|dob|\n",
      "+---+----+-----------+--------+---+-----+-----+---+\n",
      "+---+----+-----------+--------+---+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking Emanuele della Valle before removal\n",
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show()\n",
    "#removing the author \n",
    "df_authors = df_authors.filter(f.col(\"_id\") != \"638db170ae9ea0d19fad7a79\")\n",
    "#checking Emanuele della Valle after removal\n",
    "df_authors.filter(f.col(\"_id\") == \"638db170ae9ea0d19fad7a79\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958b3c73-f2a5-4ede-b584-36ee3ca38586",
   "metadata": {},
   "source": [
    "# QUERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3915e05-0e7f-4e7b-9372-c104ec1c7908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+----+----+\n",
      "|title                                                          |raw |type|\n",
      "+---------------------------------------------------------------+----+----+\n",
      "|Locality Sensitive Outlier Detection: A ranking driven approach|ICDE|0   |\n",
      "+---------------------------------------------------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+JOIN - QUERY 1\n",
    "#Print the type of the venue of an article with a specific title\n",
    "\n",
    "#Description: starting from the articles dataframe, a join is performed with the venues dataframe on the article's venue_raw field.\n",
    "#After that, we filter the articles with the given title. Finally, we project over title,venue raw and venue type.\n",
    "df_articles.join(df_venues, df_articles.venue_raw == df_venues.raw, \"inner\")\\\n",
    "           .filter(f.col(\"title\") == \"Locality Sensitive Outlier Detection: A ranking driven approach\")\\\n",
    "           .select(\"title\", \"raw\", \"type\")\\\n",
    "           .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4587ee58-c8e5-4280-8530-de0222d19c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "|Editorial: The Terminology of Machine Learning                                                                           |\n",
      "|Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005|\n",
      "|Medical Expert Evaluation of Machine Learning Results for a Coronary Heart Disease Database                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+LIMIT+LIKE - QUERY 2\n",
    "#Articles whose title string contains \"Machine Learning\" - limit 3\n",
    "\n",
    "#Description: we filter the articles whose title contains \"Machine Learning\" using the like operator. Results are then limited to 3 tuples and \n",
    "# projected over the article title.\n",
    "df_articles.filter(f.col(\"title\").like(\"%Machine Learning%\"))\\\n",
    "           .limit(3)\\\n",
    "           .select(\"title\")\\\n",
    "           .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5313e11c-f9a7-4424-8621-b25cab62b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+\n",
      "|name                    |nationality|\n",
      "+------------------------+-----------+\n",
      "|Ye Wang                 |dk         |\n",
      "|Srinivasan Parthasarathy|jp         |\n",
      "|Shirish Tatikonda       |gr         |\n",
      "|Moshe Zukerman          |jp         |\n",
      "|Michael Wiegand         |jp         |\n",
      "|GeunSik Jo              |jp         |\n",
      "|Carla Achury            |gr         |\n",
      "|Kong-Aik Lee            |jp         |\n",
      "|Shahram Shah-Heydari    |gr         |\n",
      "|Wenfang Tan             |dk         |\n",
      "|Ayoub Alsarhan          |gr         |\n",
      "|Anjali Agarwal          |jp         |\n",
      "|David Haccoun           |jp         |\n",
      "|Silvio Macedo           |dk         |\n",
      "|John Wan Tung Lee       |gr         |\n",
      "|Geoff Holmes            |dk         |\n",
      "|Zornitsa Kozareva       |jp         |\n",
      "|Peter Murray-Rust       |jp         |\n",
      "|Rajkumar Buyya          |jp         |\n",
      "|Srikumar Venugopal      |jp         |\n",
      "+------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+IN+NESTED_QUERY - QUERY 3\n",
    "#Find authors that has the same nationality of at least one of the authors of \"Locality Sensitive Outlier Detection: A ranking driven approach\" article\n",
    "\n",
    "#Description: this query has been splitted in 2 queries.\n",
    "# First query: articles are filtered to find the article with the given title. After that, the authors array is exploded to perform a join \n",
    "# on its idAuth field with the authors dataframe. Finally, nationalities of the article's authors are collected into a list using the collect_set.\n",
    "# Collect_set, as the name suggests, discards duplicates, so the final list is a set of nationalities.\n",
    "# Second query: starting from the authors' dataframe, we filter all the authors whose nationality is present inside the list created with the previous query\n",
    "\n",
    "#Create the list of nationalities of the article's authors\n",
    "nationalities_list = df_articles.filter(f.col(\"title\") == \"Locality Sensitive Outlier Detection: A ranking driven approach\")\\\n",
    "                            .select(f.explode(df_articles.authors.idAuth).alias(\"idAuth\"))\\\n",
    "                            .join(df_authors, on=f.col(\"idAuth\") == df_authors._id)\\\n",
    "                            .select(\"nationality\")\\\n",
    "                            .agg(f.collect_set(\"nationality\")).collect()[0][0]\n",
    "#find all the authors with the same nationalities of the authors of the initial article \n",
    "df_authors.filter(f.col(\"nationality\")\\\n",
    "          .isin(nationalities_list))\\\n",
    "          .select(\"name\",\"nationality\")\\\n",
    "          .show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4b9d50f-b125-4548-99ec-f9cf939c1b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+\n",
      "|        keywords|n_occurences|\n",
      "+----------------+------------+\n",
      "|     data mining|          27|\n",
      "|computer science|          22|\n",
      "|        internet|          17|\n",
      "+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GROUP_BY+JOIN+AS - QUERY 4\n",
    "#Print the 3 most frequent keywords of articles written by italian authors\n",
    "\n",
    "#Print the 3 most frequent keywords of articles written by italian authors\n",
    "#Description: starting from the authors dataframe, we keep only italian authors and explode the articles field, renaming the new obtained field\n",
    "#to \"articles\". After that, duplicates are discarded.\n",
    "#In the second part of the query, we load the full articles's rows using a join. Then, keywords array is exploded. Keywords are grouped and counted.\n",
    "#The groups are finally sorted and limited to show the top 3 keywords.\n",
    "df_italian = df_authors.filter(f.col(\"nationality\") == \"it\")\\\n",
    "                       .select(f.explode(\"articles\")).withColumnRenamed(\"col\",\"articles\")\\\n",
    "                       .distinct()\n",
    "\n",
    "df_keywords = df_italian.join(df_articles, df_italian.articles == df_articles._id, \"inner\")\\\n",
    "                        .select(\"articles\", f.explode(\"keywords\")).withColumnRenamed(\"col\",\"keywords\")\\\n",
    "                        .groupby(\"keywords\")\\\n",
    "                        .agg(f.count(\"keywords\").alias(\"n_occurences\"))\\\n",
    "                        .sort(\"n_occurences\", ascending=False)\\\n",
    "                        .limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b083d5ad-d135-40b8-aba5-c5fabf077dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    city|count|\n",
      "+--------+-----+\n",
      "|   Paris|   78|\n",
      "|Istanbul|   72|\n",
      "|  Vienna|   68|\n",
      "|    Riga|   68|\n",
      "| Tbilisi|   66|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#WHERE+GROUP_BY - QUERY 5\n",
    "#Print the cities with more than 65 venues\n",
    "\n",
    "#Description: the venues dataframe is grouped with respect to the city to perform the count. After that, we keep only cities with more than 65 venues\n",
    "#and sort the result in descending order.\n",
    "df_venues \\\n",
    "    .groupby(\"city\")\\\n",
    "    .count()\\\n",
    "    .filter(f.col(\"count\") > 65)\\\n",
    "    .sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e534aeec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+\n",
      "|fos                         |n_occurencies|\n",
      "+----------------------------+-------------+\n",
      "|Computer science            |3988         |\n",
      "|Artificial intelligence     |1246         |\n",
      "|Mathematics                 |1194         |\n",
      "|Algorithm                   |575          |\n",
      "|Computer network            |452          |\n",
      "|Computer vision             |395          |\n",
      "|Distributed computing       |388          |\n",
      "|Engineering                 |374          |\n",
      "|Pattern recognition         |333          |\n",
      "|Data mining                 |327          |\n",
      "|Theoretical computer science|326          |\n",
      "|Discrete mathematics        |294          |\n",
      "|Mathematical optimization   |293          |\n",
      "|World Wide Web              |264          |\n",
      "|Machine learning            |263          |\n",
      "|Combinatorics               |239          |\n",
      "|Control theory              |227          |\n",
      "|Information retrieval       |222          |\n",
      "|Programming language        |217          |\n",
      "|Knowledge management        |203          |\n",
      "+----------------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query 6 GROUP BY +  HAVING + AS\n",
    "#find the field of studies that appears more than 15 times\n",
    "\n",
    "#Decription: We use the explode function to convert the fos array into multiple rows,then we rename the resulting column to \"fos\",\n",
    "# group by \"fos\" and count the number of occurrences.\n",
    "# After that, we keep rows with more than 15 occurrences, sort the remaining rows in descending order based on the number of occurrences,\n",
    "# and show the top rows\n",
    "df_articles\\\n",
    "    .select(\"_id\", \"title\", f.explode(\"fos\")).withColumnRenamed(\"col\", \"fos\")\\\n",
    "    .groupby(\"fos\")\\\n",
    "    .agg(f.count(\"fos\").alias(\"n_occurencies\"))\\\n",
    "    .filter(f.col(\"n_occurencies\") > 15)\\\n",
    "    .sort(\"n_occurencies\", ascending=False)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "825e3c98",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+------------+\n",
      "|venue_raw                          |volume|num_articles|\n",
      "+-----------------------------------+------+------------+\n",
      "|Applied Mathematics and Computation|218   |5           |\n",
      "|Pattern Recognition                |45    |5           |\n",
      "|Expert Syst. Appl.                 |39    |5           |\n",
      "|Applied Mathematics and Computation|217   |5           |\n",
      "|IEICE Transactions                 |97-A  |5           |\n",
      "|Expert Syst. Appl.                 |37    |5           |\n",
      "+-----------------------------------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY7 WHERE + GROUP BY + HAVING + AS\n",
    "#Find all the volumes with at least 5 articles in this dataset published after 2000\n",
    "\n",
    "#Description: This query filters the articles in the articles dataframe to only those published after the year 2000,\n",
    "# groups the remaining articles by venue_raw and volume, \n",
    "# counts the number of articles per group, \n",
    "# filters the groups to only those with more than 4 articles, \n",
    "# and finally displays the results.\n",
    "df_articles\\\n",
    "    .filter(f.col(\"year\") > 2000)\\\n",
    "    .groupby(\"venue_raw\", \"volume\")\\\n",
    "    .agg(f.count(\"volume\").alias(\"num_articles\"))\\\n",
    "    .filter(f.col(\"num_articles\") > 4)\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce613aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----+--------------+\n",
      "|VenueRAW                                  |year|articles_count|\n",
      "+------------------------------------------+----+--------------+\n",
      "|Clinical Orthopaedics and Related Research|2010|11            |\n",
      "|Clinical Orthopaedics and Related Research|2011|8             |\n",
      "|Clinical Orthopaedics and Related Research|2009|7             |\n",
      "|Clinical Orthopaedics and Related Research|2008|5             |\n",
      "|Clinical Orthopaedics and Related Research|2007|3             |\n",
      "|Clinical Orthopaedics and Related Research|2000|2             |\n",
      "|Clinical Orthopaedics and Related Research|2012|1             |\n",
      "|Clinical Orthopaedics and Related Research|2006|1             |\n",
      "|Clinical Orthopaedics and Related Research|2013|1             |\n",
      "|Clinical Orthopaedics and Related Research|2005|1             |\n",
      "|Clinical Orthopaedics and Related Research|2001|1             |\n",
      "+------------------------------------------+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 8:  WHERE + NESTED QUERY + GROUPBY\n",
    "    #8.a)Find the venue with highest number of articles\n",
    "    #8.b)Find the number of articles published per year on that venue\n",
    "\n",
    "#Description: the first query selects the top venue from the venue dataframe based on the size of the \"artIds\" attribute.\n",
    "#The second query filters articles with the selected venue raw, groups the articles by year, and counts the number of articles in each group.\n",
    "#Finally, it displays the results projecting over top venue, year and number of articles.\n",
    "top_venue = df_venues\\\n",
    "            .select(\"raw\",f.size(\"artIds\").alias(\"count\"))\\\n",
    "            .orderBy(\"count\",ascending = False)\\\n",
    "            .limit(1)\n",
    "df_articles_year = df_articles\\\n",
    "                    .filter(f.col(\"venue_raw\") == top_venue.collect()[0][0])\\\n",
    "                    .groupBy(\"year\")\\\n",
    "                    .count()\\\n",
    "                    .orderBy(\"count\",ascending=False)\\\n",
    "                    .select(f.lit(top_venue.collect()[0][0]).alias(\"VenueRAW\"),\"year\",f.col(\"count\").alias(\"articles_count\"))\\\n",
    "                    .show(truncate=False)   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43a8b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--------------------------------------------------------------------+\n",
      "|title                                                                                                                                                                     |different_nationalities|nationalities_list                                                  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--------------------------------------------------------------------+\n",
      "|Being user-oriented: Convergences, divergences, and the potentials for systematic dialogue between disciplines and between researchers, designers, and providers          |17                     |[be, de, dk, fr, gr, hu, it, jp, nl, no, pl, pt, ro, ru, se, tr, us]|\n",
      "|A Low-Power Single-Weight-Combiner 802.11abg SoC in 0.13 Âµm CMOS for Embedded Applications Utilizing An Area and Power Efficient Cartesian Phase Shifter and Mixer Circuit|17                     |[de, dk, es, fr, gr, hu, it, jp, nl, no, pl, ro, ru, se, tr, uk, us]|\n",
      "|A 7Gb/s/pin GDDR5 SDRAM with 2.5ns bank-to-bank active time and no bank-group restriction                                                                                 |15                     |[de, dk, fr, gr, hu, it, jp, no, pl, pt, ro, ru, se, tr, uk]        |\n",
      "|Cluster Analysis and Decision Trees of MR Imaging in Patients Suffering Alzheimer's                                                                                       |14                     |[be, de, es, gr, jp, nl, no, pt, ro, ru, se, tr, uk, us]            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 9 WHERE, GROUP BY,HAVING, 1 JOIN\n",
    "#Find the articles, published after 2000, with more than 13 different nationalities of its authors\n",
    "\n",
    "#Description: this query filters the articles dataframe by year, exploding the authors array \"idAuth\" field. Note that authors is an array of struct elements with 2 fields.\n",
    "#After that, it joins the result with the authors dataframe, groups the articles by id and counts the number of distinct nationalities among the authors.\n",
    "#It finally filters the results to include only articles with more than 13 different nationalities and orders the results in descending order, displaying the title of the article, the list\n",
    "#of nationalities and their count.\n",
    "df_articles_nationalities = df_articles.alias(\"art\")\\\n",
    "                            .filter(f.col(\"year\") > 2000)\\\n",
    "                            .select(\"art._id\",\"art.title\", f.explode(\"art.authors.idAuth\").alias(\"author\"))\\\n",
    "                            .join(df_authors.alias(\"auth\"), on=f.col(\"author\") == df_authors._id)\\\n",
    "                            .groupBy(\"art._id\")\\\n",
    "                            .agg(f.first(\"title\").alias(\"title\"),f.countDistinct(\"nationality\").alias(\"different_nationalities\"), f.collect_set(\"nationality\").alias(\"nationalities_list\"))\\\n",
    "                            .filter(f.col(\"different_nationalities\") > 13)\\\n",
    "                            .orderBy(\"different_nationalities\",ascending=False)\\\n",
    "                            .select(\"title\",\"different_nationalities\",f.sort_array(\"nationalities_list\").alias(\"nationalities_list\"))\\\n",
    "                            .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "020a47f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "|_id                     |name         |venue_count|venues_list                                                                          |\n",
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "|54055740dabfae44f0803fbb|Naohiro Ishii|3          |Las Vegas, NV - Honolulu, HI - International Journal on Artificial Intelligence Tools|\n",
      "+------------------------+-------------+-----------+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 10 WHERE, GROUP BY, HAVING, 2 JOINS\n",
    "#Find all the authors that published on more than 2 Journals \n",
    "\n",
    "#Description: starting from the authors dataframe, we explode the articles array, creating a new field named \"article\".\n",
    "#After that, we join the results with the articles collection and then with the venues collection. Then, we filter the results to keep only articles written on journals (type 1) and group by\n",
    "#the author id. Finally, we count the number of distinct venues in each group (collecting in a list all the venues of the group), and keep only the groups with more than 2 venues.\n",
    "df_exploded_authors = df_authors.alias(\"auth\")\\\n",
    "                        .select(\"auth._id\",\"auth.name\", f.explode(\"auth.articles\").alias(\"article\"))\\\n",
    "                        .join(df_articles.alias(\"art\"), on=f.col(\"article\") == df_articles._id)\\\n",
    "                        .select(\"auth._id\",\"auth.name\",\"art._id\",\"art.venue_raw\")\\\n",
    "                        .join(df_venues.alias(\"ven\"), on=f.col(\"venue_raw\") == df_venues.raw)\\\n",
    "                        .filter(f.col(\"type\") == 1)\\\n",
    "                        .groupBy(\"auth._id\")\\\n",
    "                        .agg(f.first(\"name\").alias(\"name\"),f.countDistinct(\"raw\").alias(\"venue_count\"),f.concat_ws(\" - \",f.collect_set(\"raw\")).alias(\"venues_list\"))\\\n",
    "                        .filter(f.col(\"venue_count\") > 2)\\\n",
    "                        .orderBy(\"venue_count\", ascending=False).show(3,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78b4ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n",
      "|title                                                                                  |authorsList                                                                                                                                                                                                 |differentLetters|\n",
      "+---------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n",
      "|Design principles for developing stream processing applications                        |Bugra Gedik, Chitra Venkatramani, Deepak S. Turaga, Henrique Andrade, Jeffrey David Harris, John Cox, Olivier Verscheure, Paul Jones, William Szewczyk                                                      |26              |\n",
      "|Building an information retrieval test collection for spontaneous conversational speech|Bhuvana Ramabhadran, Dagobert Soergel, David S. Doermann, Douglas W. Oard, G. Craig Murray, James Mayfield, Jianqiang Wang, Liliya Kharevych, Martin Franz, Samuel Gustman, Stephanie Strassel, Xiaoli Huang|26              |\n",
      "+---------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 11\n",
    "# This query returns all the articles written by authors whose names combined have all 26 letters of the alphabet\n",
    "\n",
    "#Description: The query starts with exploding articles for each author. Then, the grouping combined with the collect retrieves for each article the list of authors \n",
    "# that have written the article, then several operation are applied on this list in order to obtain the different letters that are present in the list of authors.\n",
    "# Then, a filter to keep only the ones that have all the 26 letters of the alphabet in it is applied,\n",
    "# and the result is joined with articles to obtain the title.\n",
    "# Finally, a projection is used to display the title and the list of authors in alphabetical order.\n",
    "df_authors.select(\"name\", f.explode(\"articles\").alias(\"idArt\")) \\\n",
    "            .groupBy(\"idArt\") \\\n",
    "            .agg(f.collect_set(\"name\").alias(\"authorsList\")) \\\n",
    "            .select(\"idArt\", \"authorsList\", (f.size(f.array_distinct(f.split(f.regexp_replace(f.lower(f.concat_ws(\"\", \"authorsList\")), \"[^a-z]\", \"\"), \"\")))-1).alias(\"differentLetters\")) \\\n",
    "            .filter(f.col(\"differentLetters\") == 26)\\\n",
    "            .join(df_articles, on=f.col(\"idArt\") == df_articles._id) \\\n",
    "            .select(\"title\", f.concat_ws(\", \", f.sort_array(\"authorsList\")).alias(\"authorsList\"), \"differentLetters\") \\\n",
    "            .show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bcc88a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                                    |name                    |org                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|\"The Fire and The Mountain\": tangible and social interaction in a museum exhibition for children                         |Franca Garzotto         |Politecnico of Milano, Mlano, Italy                                                                                                   |\n",
      "|\"The Fire and The Mountain\": tangible and social interaction in a museum exhibition for children                         |Francesca Rizzo         |Politecnico of Milano, Mlano, Italy                                                                                                   |\n",
      "|A Logical Model for Agent Communication Languages                                                                        |Marco Colombetti        |Politecnico di Milano Milano, Italy University of Lugano Lugano, Switzerland                                                          |\n",
      "|A Logical Model for Agent Communication Languages                                                                        |Mario Verdicchio        |Department of Electronics and Information Politecnico di Milano Milan, Italy                                                          |\n",
      "|A posteriori dual-mixed adaptive finite element error control for LamÃ© and Stokes equations                              |Riccardo Sacco          |Dipartimento di Matematica â F. Brioschiâ, Politecnico di Milano, via Bonardi 9, 20133, Milano, Rocquencourt, Italy                   |\n",
      "|Coordinated cutting plane generation via multi-objective separation.                                                     |Edoardo Amaldi          |Dipartimento di Elettronica ed Informazione,Politecnico di Milano,Milano,Italy                                                        |\n",
      "|Coordinated cutting plane generation via multi-objective separation.                                                     |Stefano Coniglio        |Dipartimento di Elettronica ed Informazione,Politecnico di Milano,Milano,Italy                                                        |\n",
      "|Coordinated cutting plane generation via multi-objective separation.                                                     |Stefano Gualandi        |Dipartimento di Elettronica ed Informazione,Politecnico di Milano,Milano,Italy                                                        |\n",
      "|Hierarchy-based mining of association rules in data warehouses                                                           |Giuseppe Psaila         |Politecnico di Milano, Dipartimento di Elettronica e Informazione, Piazza L. Da Vinci, 32, I-20133 Milano, Italy                      |\n",
      "|Hierarchy-based mining of association rules in data warehouses                                                           |Pier Luca Lanzi         |Politecnico di Milano, Dipartimento di Elettronica e Informazione, Piazza L. Da Vinci, 32, I-20133 Milano, Italy                      |\n",
      "|ICT and mobile health to improve clinical process delivery. a research project for therapy management process innovation.|Nicola Restifo          |Fdn Politecn Milano, Milan, Italy                                                                                                     |\n",
      "|ICT and mobile health to improve clinical process delivery. a research project for therapy management process innovation.|Paolo Locatelli         |Fdn Politecn Milano, Milan, Italy                                                                                                     |\n",
      "|ICT and mobile health to improve clinical process delivery. a research project for therapy management process innovation.|Roberta Facchini        |Fdn Politecn Milano, Milan, Italy                                                                                                     |\n",
      "|Live goals for adaptive service compositions                                                                             |Liliana Pasquale        |Politecnico di Milano, Milano, Italy                                                                                                  |\n",
      "|Live goals for adaptive service compositions                                                                             |Luciano Baresi          |Politecnico di Milano, Milano, Italy                                                                                                  |\n",
      "|Parallel conjugate gradient with Schwarz preconditioner applied to fluid dynamics problems                               |A. Quarteroni           |Politecnico di Milano, P.za Leonardo da Vinci, 32, 1-20133 Milano, Italy                                                              |\n",
      "|Refining and Compressing Abstract Model Checking                                                                         |Elisa Quintarelli       |Dipartimento di Elettronica e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32, 20133 Milano, Italy                   |\n",
      "|Risk analysis of underground infrastructures in urban areas                                                              |Massimiliano De Ambroggi|Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Piazza Leonardo da Vinci 32, Milan 20132, Italy|\n",
      "|Risk analysis of underground infrastructures in urban areas                                                              |Ottavio Grande          |Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Piazza Leonardo da Vinci 32, Milan 20132, Italy|\n",
      "|Risk analysis of underground infrastructures in urban areas                                                              |Paolo Trucco            |Department of Management, Economics and Industrial Engineering, Politecnico di Milano, Piazza Leonardo da Vinci 32, Milan 20132, Italy|\n",
      "+-------------------------------------------------------------------------------------------------------------------------+------------------------+--------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#QUERY 12\n",
    "# Find all articles written in affiliation with Politecnico of Milano\n",
    "\n",
    "#Description: the query starts by exploding the authors array field in the article, creating the new \"affiliation\" attribute.\n",
    "# Articles that contain at least one of the wanted organization (the same article could be written in collaboration with different universities) are kept.\n",
    "# Then, a join with the authors collection is executed to retrieve the name of the author. \n",
    "df_articles\\\n",
    "        .select(\"title\",f.explode(\"authors\").alias(\"affiliation\"))\\\n",
    "        .filter(f.col(\"affiliation.org\").like(\"%Poli%Mil%\"))\\\n",
    "        .join(df_authors, on=f.col(\"affiliation.idAuth\") == df_authors._id) \\\n",
    "        .select(\"title\", \"name\", \"affiliation.org\") \\\n",
    "        .orderBy(\"title\",\"name\")\\\n",
    "        .show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smbud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "98f5a73632e45f3c749d517ea9db7fa37c5dacb9080f954653b66c5761e6cae0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
